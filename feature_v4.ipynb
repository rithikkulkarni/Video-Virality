{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9RA9isoBwn5d",
        "ypwBULDCHuz9",
        "Fg6G1_sAG7Kc",
        "FEeZPPvQxfWp"
      ],
      "authorship_tag": "ABX9TyOycEQ3TPqSRLzErQVYJpOu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rithikkulkarni/Video-Virality-Testing/blob/main/feature_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 0\n",
        "print(\"cell started (cache helpers)\")\n",
        "import os, gc, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "WORKDIR = Path(\"/content\")\n",
        "CACHE = WORKDIR / \"cache\"\n",
        "CACHE.mkdir(exist_ok=True)\n",
        "\n",
        "def chunker(n, batch):\n",
        "    for s in range(0, n, batch):\n",
        "        yield s, min(s+batch, n)\n",
        "\n",
        "def torch_cleanup():\n",
        "    try:\n",
        "        import torch\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "    except Exception:\n",
        "        pass\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQxCCI6R4BrC",
        "outputId": "f1167b1e-23af-4262-f5f3-b0fef5cae620"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cell started (cache helpers)\n",
            "cell complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Thumbnail Features"
      ],
      "metadata": {
        "id": "fM0DOyoBSOOz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "NI5jIkJVfPCG",
        "outputId": "2445e70e-1258-4dee-a485-dbfb7049557b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cell started\n",
            "cell complete\n"
          ]
        }
      ],
      "source": [
        "### Cell 1\n",
        "print(\"cell started\")\n",
        "filename = \"test_large.csv\"\n",
        "\n",
        "# Load the CSV\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(f'/content/{filename}')\n",
        "df.head()\n",
        "print(\"cell complete\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 2\n",
        "print(\"cell started\")\n",
        "# Extract video IDs\n",
        "video_ids = df[\"id\"].dropna().unique().tolist()\n",
        "print(f\"Found {len(video_ids)} video IDs.\")\n",
        "\n",
        "# Download thumbnails\n",
        "import os\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "os.makedirs(\"thumbnails\", exist_ok=True)\n",
        "\n",
        "def download_thumbnail(video_id):\n",
        "    url = f\"https://i.ytimg.com/vi/{video_id}/mqdefault.jpg\"\n",
        "    path = f\"thumbnails/{video_id}.jpg\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "        img.save(path)\n",
        "        return path\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {video_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "paths = [download_thumbnail(vid) for vid in video_ids]\n",
        "\n",
        "\n",
        "\n",
        "##### The below code creates a temporary thumbnail_path column that maps\n",
        "##### Each video for thumbnail referencing in cross-level features\n",
        "\n",
        "# Build a mapping from video_id -> local thumbnail path (only if file exists)\n",
        "path_map = {\n",
        "    str(vid): f\"thumbnails/{vid}.jpg\"\n",
        "    for vid in video_ids\n",
        "    if os.path.isfile(f\"thumbnails/{vid}.jpg\")\n",
        "}\n",
        "\n",
        "# Add paths to the DataFrame\n",
        "df[\"thumbnail_path\"] = df[\"id\"].astype(str).map(path_map)\n",
        "\n",
        "# Optional: quick sanity flag + count\n",
        "df[\"thumbnail_exists\"] = df[\"thumbnail_path\"].apply(\n",
        "    lambda p: isinstance(p, str) and os.path.isfile(p)\n",
        ")\n",
        "print(\"Thumbnails mapped for rows:\", int(df[\"thumbnail_exists\"].sum()), \"/\", len(df))\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "1LA2vLJMfflS",
        "outputId": "cc50bf02-935d-4b3a-8452-30313012f394",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cell started\n",
            "Found 30618 video IDs.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1052300916.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdownload_thumbnail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvideo_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1052300916.py\u001b[0m in \u001b[0;36mdownload_thumbnail\u001b[0;34m(video_id)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"thumbnails/{video_id}.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1431\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1249\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 3\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# thumbnail_colorfulness\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def compute_colorfulness(img_path):\n",
        "    \"\"\"\n",
        "    Implements the Hasler–Süsstrunk metric:\n",
        "      C = sqrt(σ_rg^2 + σ_yb^2) + 0.3 * sqrt(μ_rg^2 + μ_yb^2)\n",
        "    where\n",
        "      rg = |R − G|\n",
        "      yb = |0.5*(R + G) − B|\n",
        "    \"\"\"\n",
        "    img = np.array(Image.open(img_path).convert('RGB')).astype('float32')\n",
        "    R, G, B = img[...,0], img[...,1], img[...,2]\n",
        "    rg = np.abs(R - G)\n",
        "    yb = np.abs(0.5*(R + G) - B)\n",
        "\n",
        "    std_rg = np.std(rg)\n",
        "    std_yb = np.std(yb)\n",
        "    mean_rg = np.mean(rg)\n",
        "    mean_yb = np.mean(yb)\n",
        "\n",
        "    # Hasler–Süsstrunk colorfulness\n",
        "    return np.sqrt(std_rg**2 + std_yb**2) + 0.3 * np.sqrt(mean_rg**2 + mean_yb**2)\n",
        "\n",
        "# Assume `df` has an \"id\" column of video_ids and your thumbnails are saved as thumbnails/{id}.jpg\n",
        "colorfulness_scores = []\n",
        "for vid in df[\"id\"].dropna().unique():\n",
        "    path = os.path.join(\"thumbnails\", f\"{vid}.jpg\")\n",
        "    if os.path.exists(path):\n",
        "        colorfulness_scores.append((vid, compute_colorfulness(path)))\n",
        "    else:\n",
        "        colorfulness_scores.append((vid, np.nan))\n",
        "\n",
        "# Turn into a dict for fast lookup, then map back onto df\n",
        "cf_dict = dict(colorfulness_scores)\n",
        "df[\"thumbnail_colorfulness\"] = df[\"id\"].map(cf_dict)\n",
        "\n",
        "# Inspect\n",
        "df[[\"id\", \"thumbnail_colorfulness\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "bi05J_2M6jOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 4\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# thumbnail_brightness\n",
        "# thumbnail_contrast\n",
        "\n",
        "def compute_brightness_rms(img_path):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      - mean_luminance: Mean pixel intensity (0 → 255)\n",
        "      - rms_contrast: Standard deviation of luminance divided by mean luminance\n",
        "    \"\"\"\n",
        "    img = np.array(Image.open(img_path).convert('L')).astype(np.float32)\n",
        "    mean_luminance = img.mean()\n",
        "    rms_contrast = img.std() / mean_luminance if mean_luminance != 0 else 0.0\n",
        "    return mean_luminance, rms_contrast\n",
        "\n",
        "# Compute brightness & RMS contrast for each video\n",
        "results = []\n",
        "for vid in df[\"id\"].dropna().unique():\n",
        "    thumb_path = os.path.join(\"thumbnails\", f\"{vid}.jpg\")\n",
        "    if os.path.isfile(thumb_path):\n",
        "        brightness, rms = compute_brightness_rms(thumb_path)\n",
        "    else:\n",
        "        brightness, rms = np.nan, np.nan\n",
        "    results.append((vid, brightness, rms))\n",
        "\n",
        "# Map back to DataFrame\n",
        "brightness_map = {vid: b for vid, b, _ in results}\n",
        "rms_map        = {vid: rms for vid, _, rms in results}\n",
        "\n",
        "df[\"thumbnail_brightness\"]   = df[\"id\"].map(brightness_map)\n",
        "df[\"thumbnail_contrast\"] = df[\"id\"].map(rms_map)\n",
        "\n",
        "# Preview updated columns\n",
        "df[[\"id\", \"thumbnail_brightness\", \"thumbnail_contrast\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "KsZ6mSRE7aY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 5\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# thumbnail_hue\n",
        "# thumbnail_saturation\n",
        "\n",
        "def compute_hue_saturation(img_path):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      - mean_hue: Mean hue in degrees (0 → 360)\n",
        "      - mean_saturation: Mean saturation (0 → 1)\n",
        "    \"\"\"\n",
        "    # PIL HSV channels are 0–255; convert to float\n",
        "    img = np.array(Image.open(img_path).convert('HSV')).astype(np.float32)\n",
        "    H, S, _ = img[..., 0], img[..., 1], img[..., 2]\n",
        "    # Scale H to degrees, S to fraction\n",
        "    mean_hue = (H.mean() * 360.0) / 255.0\n",
        "    mean_saturation = S.mean() / 255.0\n",
        "    return mean_hue, mean_saturation\n",
        "\n",
        "# Compute hue & saturation features for each video\n",
        "results = []\n",
        "for vid in df[\"id\"].dropna().unique():\n",
        "    thumb_path = os.path.join(\"thumbnails\", f\"{vid}.jpg\")\n",
        "    if os.path.isfile(thumb_path):\n",
        "        hue, sat = compute_hue_saturation(thumb_path)\n",
        "    else:\n",
        "        hue, sat = np.nan, np.nan\n",
        "    results.append((vid, hue, sat))\n",
        "\n",
        "# Map back to DataFrame\n",
        "hue_map = {vid: hue for vid, hue, _ in results}\n",
        "sat_map = {vid: sat for vid, _, sat in results}\n",
        "\n",
        "df[\"thumbnail_hue\"] = df[\"id\"].map(hue_map)\n",
        "df[\"thumbnail_saturation\"] = df[\"id\"].map(sat_map)\n",
        "\n",
        "# Preview new columns\n",
        "df[[\"id\", \"thumbnail_hue\", \"thumbnail_saturation\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "IA9s2t9s94bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 6\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# thumbnail_edge_density\n",
        "# thumbnail_texture_entropy\n",
        "\n",
        "import cv2\n",
        "\n",
        "def compute_edge_texture(img_path):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      - edge_density: fraction of Canny edge pixels (0 → 1)\n",
        "      - texture_entropy: Shannon entropy of grayscale pixel distribution (in bits)\n",
        "    \"\"\"\n",
        "    # Load as grayscale\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    # Edge density\n",
        "    edges = cv2.Canny(img, threshold1=100, threshold2=200)\n",
        "    edge_density = np.count_nonzero(edges) / img.size\n",
        "\n",
        "    # Texture (grayscale) entropy\n",
        "    hist, _ = np.histogram(img.flatten(), bins=256, range=(0, 255))\n",
        "    probs = hist / hist.sum()\n",
        "    probs_nonzero = probs[probs > 0]\n",
        "    texture_entropy = -np.sum(probs_nonzero * np.log2(probs_nonzero))\n",
        "\n",
        "    return edge_density, texture_entropy\n",
        "\n",
        "# Compute edge density & texture entropy for each video\n",
        "results = []\n",
        "for vid in df[\"id\"].dropna().unique():\n",
        "    thumb_path = os.path.join(\"thumbnails\", f\"{vid}.jpg\")\n",
        "    if os.path.isfile(thumb_path):\n",
        "        ed, te = compute_edge_texture(thumb_path)\n",
        "    else:\n",
        "        ed, te = np.nan, np.nan\n",
        "    results.append((vid, ed, te))\n",
        "\n",
        "# Map results back to DataFrame\n",
        "edge_density_map   = {vid: ed for vid, ed, _ in results}\n",
        "texture_entropy_map = {vid: te for vid, _, te in results}\n",
        "\n",
        "df[\"thumbnail_edge_density\"]    = df[\"id\"].map(edge_density_map)\n",
        "df[\"thumbnail_texture_entropy\"] = df[\"id\"].map(texture_entropy_map)\n",
        "\n",
        "# Preview new columns\n",
        "df[[\"id\", \"thumbnail_edge_density\", \"thumbnail_texture_entropy\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "KbLRUDHE-cT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 7\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# thumbnail_quality\n",
        "\n",
        "def compute_thumbnail_quality(img_path):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      - thumbnail_quality: variance of the Laplacian of the grayscale image\n",
        "        (higher = sharper, lower = blurrier)\n",
        "    \"\"\"\n",
        "    gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    lap = cv2.Laplacian(gray, cv2.CV_64F)\n",
        "    return lap.var()\n",
        "\n",
        "# Compute thumbnail_quality for each video\n",
        "results = []\n",
        "for vid in df[\"id\"].dropna().unique():\n",
        "    thumb_path = os.path.join(\"thumbnails\", f\"{vid}.jpg\")\n",
        "    if os.path.isfile(thumb_path):\n",
        "        quality = compute_thumbnail_quality(thumb_path)\n",
        "    else:\n",
        "        quality = np.nan\n",
        "    results.append((vid, quality))\n",
        "\n",
        "# Map back to DataFrame\n",
        "quality_map = {vid: q for vid, q in results}\n",
        "df[\"thumbnail_quality\"] = df[\"id\"].map(quality_map)\n",
        "\n",
        "# Preview the new feature\n",
        "df[[\"id\", \"thumbnail_quality\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "4cARFMkv_WHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 8\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# thumbnail_face_area_ratio\n",
        "\n",
        "# Load OpenCV's built-in Haar cascade for face detection\n",
        "face_cascade = cv2.CascadeClassifier(\n",
        "    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
        ")\n",
        "\n",
        "def compute_face_area_ratio(img_path):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      - face_area_ratio: sum of detected face bounding-box areas divided by total image area\n",
        "    \"\"\"\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        return np.nan\n",
        "    height, width = img.shape[:2]\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(\n",
        "        gray,\n",
        "        scaleFactor=1.1,\n",
        "        minNeighbors=5,\n",
        "        minSize=(30, 30)\n",
        "    )\n",
        "    # Sum up face areas\n",
        "    total_face_area = sum((fw * fh) for (_, _, fw, fh) in faces)\n",
        "    total_area = width * height\n",
        "    return total_face_area / total_area if total_area > 0 else 0.0\n",
        "\n",
        "# Compute face_area_ratio for each video\n",
        "results = []\n",
        "for vid in df[\"id\"].dropna().unique():\n",
        "    thumb_path = os.path.join(\"thumbnails\", f\"{vid}.jpg\")\n",
        "    ratio = compute_face_area_ratio(thumb_path) if os.path.isfile(thumb_path) else np.nan\n",
        "    results.append((vid, ratio))\n",
        "\n",
        "# Map back to DataFrame\n",
        "face_area_map = {vid: ratio for vid, ratio in results}\n",
        "df[\"thumbnail_face_area_ratio\"] = df[\"id\"].map(face_area_map)\n",
        "\n",
        "# Preview the new feature\n",
        "df[[\"id\", \"thumbnail_face_area_ratio\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "vzUU0pRhBnMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 9\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# thumbnail_face_emotion\n",
        "\n",
        "!pip install -q mediapipe==0.10.14\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "\n",
        "# Assumes `df` is already loaded and thumbnails are in \"thumbnails/{video_id}.jpg\"\n",
        "\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "def _euclid(p1, p2):\n",
        "    return float(np.hypot(p1[0]-p2[0], p1[1]-p2[1]))\n",
        "\n",
        "def _safe_ratio(a, b, eps=1e-6):\n",
        "    return float(a / (b + eps))\n",
        "\n",
        "def compute_thumbnail_face_emotion(img_path: str) -> float:\n",
        "    \"\"\"\n",
        "    Landmark-based valence proxy in [-1, 1]:\n",
        "      + Increases with mouth width (smile) and eye openness\n",
        "      + Slightly increases with mouth openness (surprise)\n",
        "      Returns 0.0 if no faces detected.\n",
        "    \"\"\"\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        return np.nan\n",
        "    H, W = img_bgr.shape[:2]\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    with mp_face_mesh.FaceMesh(\n",
        "        static_image_mode=True,\n",
        "        max_num_faces=10,\n",
        "        refine_landmarks=True,\n",
        "        min_detection_confidence=0.5\n",
        "    ) as fm:\n",
        "        res = fm.process(img_rgb)\n",
        "\n",
        "    if not res.multi_face_landmarks:\n",
        "        return 0.0\n",
        "\n",
        "    vals, areas = [], []\n",
        "    for lm in res.multi_face_landmarks:\n",
        "        pts = [(int(p.x * W), int(p.y * H)) for p in lm.landmark]\n",
        "\n",
        "        # Landmark indices (MediaPipe Face Mesh):\n",
        "        # Mouth corners: 61 (left), 291 (right)\n",
        "        # Upper/Lower lip center: 13, 14\n",
        "        # Face \"width\": 234 (left cheek), 454 (right cheek)\n",
        "        # Face \"height\": 10 (forehead), 152 (chin)\n",
        "        # Eyes: left top/bottom 159/145 with corners 33/133; right 386/374 with corners 362/263\n",
        "        face_w  = _euclid(pts[234], pts[454])\n",
        "        face_h  = _euclid(pts[10],  pts[152])\n",
        "        if face_w < 1 or face_h < 1:\n",
        "            continue\n",
        "\n",
        "        mouth_w     = _euclid(pts[61],  pts[291])\n",
        "        mouth_open  = _euclid(pts[13],  pts[14])\n",
        "        left_eye_op = _safe_ratio(_euclid(pts[159], pts[145]), _euclid(pts[33],  pts[133]))\n",
        "        right_eye_op= _safe_ratio(_euclid(pts[386], pts[374]), _euclid(pts[362], pts[263]))\n",
        "        eye_open    = 0.5 * (left_eye_op + right_eye_op)\n",
        "\n",
        "        # Normalize by face size\n",
        "        wide  = _safe_ratio(mouth_w, face_w)      # smile width\n",
        "        opened= _safe_ratio(mouth_open, face_h)   # mouth open\n",
        "        # Simple weighted score → squashed to [-1, 1]\n",
        "        score = 1.2 * wide + 0.6 * opened + 0.3 * eye_open\n",
        "        val   = float(np.tanh((score - 0.55) * 3.0))  # center and scale\n",
        "\n",
        "        # Approx area to weight bigger faces more\n",
        "        areas.append(face_w * face_h)\n",
        "        vals.append(val)\n",
        "\n",
        "    if not vals:\n",
        "        return 0.0\n",
        "    return float(np.average(vals, weights=np.asarray(areas, dtype=np.float32)))\n",
        "\n",
        "# Compute for each video and map back to df\n",
        "results = []\n",
        "for vid in df[\"id\"].dropna().unique():\n",
        "    p = os.path.join(\"thumbnails\", f\"{vid}.jpg\")\n",
        "    v = compute_thumbnail_face_emotion(p) if os.path.isfile(p) else np.nan\n",
        "    results.append((vid, v))\n",
        "\n",
        "df[\"thumbnail_face_emotion\"] = df[\"id\"].map({vid: v for vid, v in results})\n",
        "\n",
        "# Peek\n",
        "df[[\"id\", \"thumbnail_face_emotion\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "q7wf5SVlG3Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 10\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# thumbnail_ocr_text_coverage\n",
        "\n",
        "# Heuristic OCR Text Coverage (no large models)\n",
        "# Idea: enhance stroke-like regions with morphology, binarize, filter CCs, fill boxes -> coverage ∈ [0,1]\n",
        "\n",
        "def compute_text_coverage(img_path: str,\n",
        "                          min_frac=0.0005,   # min box area as fraction of image area\n",
        "                          max_frac=0.25,     # max box area as fraction of image area\n",
        "                          min_ar=1.1,        # min aspect ratio (w/h) for text regions\n",
        "                          extent_lo=0.25,    # min contour extent (area / bbox area)\n",
        "                          extent_hi=0.95):   # max contour extent (filters solid bars/blocks)\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        return np.nan\n",
        "    H, W = img.shape[:2]\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # --- Enhance text strokes ---\n",
        "    # Use both blackhat (light text on dark) and tophat (dark text on light) with an elongated kernel\n",
        "    kx = max(3, W // 40)   # horizontal emphasis\n",
        "    ky = max(2, H // 120)  # thin vertical thickness\n",
        "    rect = cv2.getStructuringElement(cv2.MORPH_RECT, (kx, ky))\n",
        "    blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, rect)\n",
        "    tophat   = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT,   rect)\n",
        "    enhanced = cv2.max(blackhat, tophat)\n",
        "\n",
        "    # Optional slight blur to stabilize thresholding\n",
        "    enhanced = cv2.GaussianBlur(enhanced, (3, 3), 0)\n",
        "\n",
        "    # --- Binarize (Otsu) ---\n",
        "    _, bw = cv2.threshold(enhanced, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # --- Connect characters into words/blocks ---\n",
        "    connect = cv2.getStructuringElement(cv2.MORPH_RECT, (max(3, kx // 2), max(2, ky)))\n",
        "    bw = cv2.morphologyEx(bw, cv2.MORPH_CLOSE, connect, iterations=1)\n",
        "    bw = cv2.dilate(bw, connect, iterations=1)\n",
        "\n",
        "    # --- Find candidate regions ---\n",
        "    contours, _ = cv2.findContours(bw, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    mask = np.zeros((H, W), dtype=np.uint8)\n",
        "    total_px = H * W\n",
        "    min_area = min_frac * total_px\n",
        "    max_area = max_frac * total_px\n",
        "\n",
        "    for cnt in contours:\n",
        "        x, y, w, h = cv2.boundingRect(cnt)\n",
        "        area = w * h\n",
        "        if area < min_area or area > max_area:\n",
        "            continue\n",
        "        ar = w / float(h) if h > 0 else 0.0\n",
        "        if ar < min_ar:\n",
        "            continue\n",
        "        cnt_area = cv2.contourArea(cnt)\n",
        "        extent = (cnt_area / area) if area > 0 else 0.0\n",
        "        if not (extent_lo <= extent <= extent_hi):\n",
        "            continue\n",
        "\n",
        "        # Fill the box (mask union of text-like regions)\n",
        "        cv2.rectangle(mask, (x, y), (x + w, y + h), 255, -1)\n",
        "\n",
        "    coverage = (mask.sum() / 255.0) / float(total_px) if total_px > 0 else 0.0\n",
        "    return float(max(0.0, min(1.0, coverage)))\n",
        "\n",
        "# Compute for each video and map back to df\n",
        "results = []\n",
        "for vid in df[\"id\"].dropna().unique():\n",
        "    p = os.path.join(\"thumbnails\", f\"{vid}.jpg\")\n",
        "    cov = compute_text_coverage(p) if os.path.isfile(p) else np.nan\n",
        "    results.append((vid, cov))\n",
        "\n",
        "df[\"thumbnail_ocr_text_coverage\"] = df[\"id\"].map({vid: cov for vid, cov in results})\n",
        "\n",
        "# Peek\n",
        "df[[\"id\", \"thumbnail_ocr_text_coverage\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "IJpB1omPK-94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 11\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# thumbnail_saliency_thirds_proximity\n",
        "\n",
        "def _spectral_residual_centroid(gray: np.ndarray, small=96):\n",
        "    \"\"\"\n",
        "    Compute saliency via spectral residual and return centroid (cx, cy) in ORIGINAL image coords.\n",
        "    \"\"\"\n",
        "    H, W = gray.shape[:2]\n",
        "    # Downscale for speed\n",
        "    small_w = small\n",
        "    small_h = max(8, int(round(H * (small / float(W))))) if W > 0 else small\n",
        "    g = cv2.resize(gray, (small_w, small_h), interpolation=cv2.INTER_AREA).astype(np.float32)\n",
        "\n",
        "    # Spectral residual saliency (Hou & Zhang 2007)\n",
        "    F = np.fft.fft2(g)\n",
        "    log_amp = np.log(np.abs(F) + 1e-8)\n",
        "    phase   = np.angle(F)\n",
        "    avg_log = cv2.blur(log_amp, (3, 3))\n",
        "    spec_res = log_amp - avg_log\n",
        "    S = np.abs(np.fft.ifft2(np.exp(spec_res + 1j * phase))) ** 2\n",
        "    S = cv2.GaussianBlur(S, (3, 3), 0)\n",
        "\n",
        "    # Normalize to [0,1]\n",
        "    S -= S.min()\n",
        "    S /= (S.max() + 1e-8)\n",
        "\n",
        "    # Centroid on the small map\n",
        "    yy, xx = np.mgrid[0:small_h, 0:small_w]\n",
        "    w = S.astype(np.float32)\n",
        "    wsum = float(w.sum())\n",
        "    if wsum < 1e-8:\n",
        "        # Fallback to image center if saliency is degenerate\n",
        "        return W * 0.5, H * 0.5\n",
        "\n",
        "    cx_small = float((w * xx).sum() / wsum)\n",
        "    cy_small = float((w * yy).sum() / wsum)\n",
        "\n",
        "    # Map back to original coords\n",
        "    cx = cx_small * (W / float(small_w))\n",
        "    cy = cy_small * (H / float(small_h))\n",
        "    return cx, cy\n",
        "\n",
        "def _thirds_points(W, H):\n",
        "    return [\n",
        "        (W/3.0, H/3.0),\n",
        "        (2*W/3.0, H/3.0),\n",
        "        (W/3.0, 2*H/3.0),\n",
        "        (2*W/3.0, 2*H/3.0),\n",
        "    ]\n",
        "\n",
        "def _nearest_thirds_distance(cx, cy, W, H):\n",
        "    pts = _thirds_points(W, H)\n",
        "    return float(min(np.hypot(cx - x, cy - y) for (x, y) in pts))\n",
        "\n",
        "def compute_thirds_proximity(img_path: str) -> float:\n",
        "    \"\"\"\n",
        "    Returns thirds proximity in [0,1]:\n",
        "      1.0 = saliency centroid exactly on a rule-of-thirds hotspot\n",
        "      0.0 ≈ worst case (near a corner)\n",
        "    \"\"\"\n",
        "    bgr = cv2.imread(img_path)\n",
        "    if bgr is None:\n",
        "        return np.nan\n",
        "    H, W = bgr.shape[:2]\n",
        "    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    cx, cy = _spectral_residual_centroid(gray, small=96)\n",
        "    diag = np.hypot(W, H)\n",
        "    max_min_dist = diag / 3.0  # worst-case distance to nearest thirds point (corners)\n",
        "    offset_px = _nearest_thirds_distance(cx, cy, W, H)\n",
        "    proximity = 1.0 - (offset_px / (max_min_dist + 1e-8))\n",
        "    return float(np.clip(proximity, 0.0, 1.0))\n",
        "\n",
        "# Compute and map back to df\n",
        "results = []\n",
        "for vid in df[\"id\"].dropna().unique():\n",
        "    p = os.path.join(\"thumbnails\", f\"{vid}.jpg\")\n",
        "    prox = compute_thirds_proximity(p) if os.path.isfile(p) else np.nan\n",
        "    results.append((vid, prox))\n",
        "\n",
        "df[\"thumbnail_saliency_thirds_proximity\"] = df[\"id\"].map({vid: prox for vid, prox in results})\n",
        "\n",
        "# Quick peek\n",
        "df[[\"id\", \"thumbnail_saliency_thirds_proximity\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "QIPcm8B-Nyd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 12\n",
        "print(\"cell started\")\n",
        "# Column renaming\n",
        "\n",
        "df.rename(columns={'snippet.channelId': 'channel_id',\n",
        "                   'snippet.title': 'title',\n",
        "                   'snippet.description': 'description',\n",
        "                   'statistics.likeCount': 'likeCount',\n",
        "                   'statistics.viewCount': 'viewCount',\n",
        "                   'statistics.commentCount': 'commentCount'}, inplace=True)\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "zsx90MJcPMqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 13\n",
        "print(\"cell started\")\n",
        "##### Labeling virality (based on high performance)\n",
        "\n",
        "percentile = 0.75\n",
        "\n",
        "# 0) Drop duplicate-named columns (keep first occurrence)\n",
        "dupes = df.columns[df.columns.duplicated()].unique()\n",
        "if len(dupes):\n",
        "    print(\"Dropping duplicate columns:\", list(dupes))\n",
        "df = df.loc[:, ~df.columns.duplicated()]\n",
        "\n",
        "# 1) Make sure metrics are numeric (YouTube API often gives strings)\n",
        "num_cols = [\"viewCount\", \"likeCount\", \"commentCount\"]\n",
        "df[num_cols] = df[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "# 2) Compute per-channel quantile thresholds\n",
        "thresholds = (\n",
        "    df.groupby(\"channel_id\")[num_cols]\n",
        "      .quantile(percentile)\n",
        "      .rename(columns={\n",
        "          \"viewCount\": \"views_threshold\",\n",
        "          \"likeCount\": \"likes_threshold\",\n",
        "          \"commentCount\": \"comments_threshold\",\n",
        "      })\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# 3) Merge thresholds into main dataframe\n",
        "df = df.merge(thresholds, on=\"channel_id\", how=\"left\", validate=\"m:1\")\n",
        "\n",
        "# 4) Label virality\n",
        "df[\"viral_label\"] = (\n",
        "    (df[\"viewCount\"] > df[\"views_threshold\"]) &\n",
        "    (df[\"likeCount\"] > df[\"likes_threshold\"]) &\n",
        "    (df[\"commentCount\"] > df[\"comments_threshold\"])\n",
        ").astype(int)\n",
        "\n",
        "# 5) Clean up\n",
        "df = df.drop(columns=[\"views_threshold\", \"likes_threshold\", \"comments_threshold\"])\n",
        "\n",
        "# 6) Debug: what's the share of viral?\n",
        "viral_percentage = (df[\"viral_label\"].sum() / len(df)) * 100 if len(df) else 0.0\n",
        "print(f\"Percentage of viral videos: {viral_percentage:.2f}%\")\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "6KaKgMSTPIY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 14\n",
        "print(\"cell started\")\n",
        "##### Dropping columns that aren't used from youtube data api v3 metadata\n",
        "##### General refactoring at end of cell\n",
        "\n",
        "# Dropping bad columns/features\n",
        "columns_to_drop = ['kind',\n",
        "                   'etag',\n",
        "                   'channel_id_x',\n",
        "                   'snippet.publishedAt',\n",
        "                   'snippet.title',\n",
        "                   'snippet.thumbnails.default.url',\n",
        "                   'snippet.thumbnails.default.width',\n",
        "                   'snippet.thumbnails.default.height',\n",
        "                   'snippet.thumbnails.medium.url',\n",
        "                   'snippet.thumbnails.medium.width',\n",
        "                   'snippet.thumbnails.medium.height',\n",
        "                   'snippet.thumbnails.high.url',\n",
        "                   'snippet.thumbnails.high.width',\n",
        "                   'snippet.thumbnails.high.height',\n",
        "                   'snippet.thumbnails.standard.url',\n",
        "                   'snippet.thumbnails.standard.width',\n",
        "                   'snippet.thumbnails.standard.height',\n",
        "                   'snippet.thumbnails.maxres.url',\n",
        "                   'snippet.thumbnails.maxres.width',\n",
        "                   'snippet.thumbnails.maxres.height',\n",
        "                   'statistics.viewCount',\n",
        "                   'statistics.likeCount',\n",
        "                   'statistics.commentCount',\n",
        "                   'snippet.channelTitle',\n",
        "                   'snippet.categoryId',\n",
        "                   'snippet.liveBroadcastContent',\n",
        "                   'snippet.defaultAudioLanguage',\n",
        "                   'snippet.defaultLanguage',\n",
        "                   'title',\n",
        "                   'channel_id_x',\n",
        "                   'channel_id_y',\n",
        "                   'snippet.localized.description',\n",
        "                   'statistics.favoriteCount',\n",
        "                   'id',\n",
        "                   'viewCount',\n",
        "                   'likeCount',\n",
        "                   'commentCount',\n",
        "                   'channel_id',\n",
        "                   'description']\n",
        "\n",
        "# Check which columns exist in the DataFrame before dropping\n",
        "existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
        "\n",
        "df = df.drop(columns=existing_columns_to_drop, axis=1)\n",
        "\n",
        "# Renaming tags for future section\n",
        "df.rename(columns={'snippet.tags': 'tags', 'snippet.localized.title': 'title'}, inplace=True)\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "l_z6leRcgDJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 15\n",
        "print(\"cell started\")\n",
        "##### Sanity check + feature peek for next section\n",
        "df.columns\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "rB7XYrRMQpUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Title-level Features"
      ],
      "metadata": {
        "id": "CGim7Ys5tv1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 16\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# title_sentiment\n",
        "\n",
        "# Install and import VADER\n",
        "!pip install -q vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize analyzer once\n",
        "_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def vader_compound(text: str) -> float:\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return np.nan  # or 0.0 if you prefer neutral for missing\n",
        "    return _analyzer.polarity_scores(text)[\"compound\"]\n",
        "\n",
        "# Compute and add the feature\n",
        "df[\"title_sentiment\"] = df[\"title\"].astype(str).map(vader_compound)\n",
        "\n",
        "# Feature preview\n",
        "df[[\"title\", \"title_sentiment\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "fHL88SE4t2G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 17\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# title_emotion_anger\n",
        "# title_emotion_disgust\n",
        "# title_emotion_fear\n",
        "# title_emotion_joy\n",
        "# title_emotion_neutral\n",
        "# title_emotion_sadness\n",
        "# title_emotion_surprise\n",
        "\n",
        "# Install transformers if missing\n",
        "try:\n",
        "    from transformers import pipeline\n",
        "except Exception:\n",
        "    !pip install -q transformers\n",
        "    from transformers import pipeline\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize emotion pipeline\n",
        "# ['anger','disgust','fear','joy','neutral','sadness','surprise']\n",
        "emo_nlp = pipeline(\n",
        "    task=\"text-classification\",\n",
        "    model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
        "    return_all_scores=True,           # get full distribution\n",
        "    device=-1                         # CPU\n",
        ")\n",
        "\n",
        "EMO_LABELS = [\"anger\",\"disgust\",\"fear\",\"joy\",\"neutral\",\"sadness\",\"surprise\"]\n",
        "NEW_COLS = [f\"title_emotion_{e}\" for e in EMO_LABELS]\n",
        "\n",
        "# Prepare inputs (keep index to map results back cleanly)\n",
        "titles = df[\"title\"].astype(str).fillna(\"\")\n",
        "idx = titles.index.to_list()\n",
        "\n",
        "def _chunks(seq, n):\n",
        "    for i in range(0, len(seq), n):\n",
        "        yield seq[i:i+n]\n",
        "\n",
        "# Run in batches to avoid memory spikes\n",
        "all_scores = []\n",
        "batch_size = 64\n",
        "for batch in _chunks(titles.tolist(), batch_size):\n",
        "    out = emo_nlp(batch, truncation=True)\n",
        "    # 'out' is a list (len=batch) of lists of dicts [{'label':..., 'score':...}, ...]\n",
        "    for per in out:\n",
        "        d = {item[\"label\"].lower(): float(item[\"score\"]) for item in per}\n",
        "        # ensure we have all 7 labels in consistent order\n",
        "        all_scores.append([d.get(k, 0.0) for k in EMO_LABELS])\n",
        "\n",
        "# Convert to DataFrame and attach to df\n",
        "scores_arr = np.array(all_scores, dtype=np.float32)\n",
        "emo_df = pd.DataFrame(scores_arr, columns=NEW_COLS, index=idx)\n",
        "\n",
        "# (Optional) ensure each row sums ~1 (numerical drift guard)\n",
        "row_sums = emo_df.sum(axis=1).replace(0, np.nan)\n",
        "emo_df = emo_df.div(row_sums, axis=0).fillna(0.0)\n",
        "\n",
        "# Merge columns into your df\n",
        "df[NEW_COLS] = emo_df[NEW_COLS]\n",
        "\n",
        "# Feature preview\n",
        "df[[\"title\"] + NEW_COLS].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "3VgfyILquHBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 18\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# title_subjectivity\n",
        "\n",
        "# Install TextBlob\n",
        "!pip install -q textblob\n",
        "\n",
        "from textblob import TextBlob\n",
        "import numpy as np\n",
        "\n",
        "def blob_subjectivity(text: str) -> float:\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return np.nan  # or 0.0 if you want to treat empty titles as neutral\n",
        "    try:\n",
        "        return float(TextBlob(text).sentiment.subjectivity)\n",
        "    except Exception:\n",
        "        # If you ever hit a pattern-related issue, try:\n",
        "        # !pip install -q \"textblob==0.17.1\" \"pattern==3.6\"\n",
        "        return np.nan\n",
        "\n",
        "# Compute and attach the feature\n",
        "df[\"title_subjectivity\"] = df[\"title\"].astype(str).map(blob_subjectivity)\n",
        "\n",
        "# Feature preview\n",
        "df[[\"title\", \"title_subjectivity\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "3Ad6OQj6ueSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 19\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# title_readability\n",
        "\n",
        "# Install textstat if needed\n",
        "!pip install -q textstat\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import textstat\n",
        "\n",
        "def flesch_reading_ease_clipped(text: str) -> float:\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return np.nan\n",
        "    try:\n",
        "        score = textstat.flesch_reading_ease(text)\n",
        "        # FRE can be <0 or >100; clamp to 0..100 for a clean feature\n",
        "        return float(np.clip(score, 0, 100))\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "# Compute features\n",
        "titles = df[\"title\"].astype(str)\n",
        "df[\"title_readability\"] = titles.map(flesch_reading_ease_clipped)   # 0..100 (higher = easier)\n",
        "\n",
        "# Feature preview\n",
        "df[[\"title\", \"title_readability\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "jW7rlLlSuxLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 20\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# title_log_perplexity\n",
        "\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "except Exception:\n",
        "    !pip install -q transformers\n",
        "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "MODEL_NAME = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "model.eval()\n",
        "\n",
        "# GPT-2 has no pad token; use EOS as pad so we can batch/pad\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def gpt2_log_perplexity_batch(texts, max_length=128):\n",
        "    \"\"\"\n",
        "    Returns mean negative log-likelihood per token (nats/token) for each text.\n",
        "    Empty/whitespace → np.nan.\n",
        "    \"\"\"\n",
        "    # Mask empties upfront\n",
        "    mask_nonempty = [isinstance(t, str) and t.strip() != \"\" for t in texts]\n",
        "    results = [np.nan] * len(texts)\n",
        "    if not any(mask_nonempty):\n",
        "        return results\n",
        "\n",
        "    nonempty = [t for t, ok in zip(texts, mask_nonempty) if ok]\n",
        "\n",
        "    enc = tokenizer(\n",
        "        nonempty,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "    )\n",
        "    input_ids = enc[\"input_ids\"].to(device)          # [B,T]\n",
        "    attention = enc[\"attention_mask\"].to(device)     # [B,T]\n",
        "\n",
        "    # Shift for causal LM loss\n",
        "    logits = model(input_ids=input_ids, attention_mask=attention).logits  # [B,T,V]\n",
        "    shift_logits = logits[:, :-1, :].contiguous()       # [B,T-1,V]\n",
        "    shift_labels = input_ids[:, 1:].contiguous()        # [B,T-1]\n",
        "    shift_attn   = attention[:, 1:].contiguous()        # [B,T-1]\n",
        "\n",
        "    # Mask pads as ignore_index\n",
        "    shift_labels = shift_labels.masked_fill(shift_attn == 0, -100)\n",
        "\n",
        "    # Per-token NLL with ignore_index; shape [B*(T-1)]\n",
        "    loss_flat = F.cross_entropy(\n",
        "        shift_logits.view(-1, shift_logits.size(-1)),\n",
        "        shift_labels.view(-1),\n",
        "        ignore_index=-100,\n",
        "        reduction=\"none\"\n",
        "    )\n",
        "    # Reshape to [B, T-1]\n",
        "    loss_tok = loss_flat.view(shift_labels.size())\n",
        "\n",
        "    # Mean over valid tokens per sequence\n",
        "    valid_mask = (shift_labels != -100).float()\n",
        "    tok_counts = valid_mask.sum(dim=1)                        # [B]\n",
        "    sum_loss   = (loss_tok * valid_mask).sum(dim=1)           # [B]\n",
        "    mean_nll   = torch.where(tok_counts > 0, sum_loss / tok_counts, torch.full_like(tok_counts, float(\"nan\")))\n",
        "\n",
        "    # Stitch back to original order\n",
        "    per_seq = mean_nll.detach().cpu().numpy().tolist()\n",
        "    it = iter(per_seq)\n",
        "    for i, ok in enumerate(mask_nonempty):\n",
        "        if ok:\n",
        "            results[i] = next(it)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Compute for your titles\n",
        "titles = df[\"title\"].astype(str).tolist()\n",
        "batch_size = 64\n",
        "logp = []\n",
        "for i in range(0, len(titles), batch_size):\n",
        "    logp.extend(gpt2_log_perplexity_batch(titles[i:i+batch_size], max_length=128))\n",
        "\n",
        "df[\"title_log_perplexity\"] = logp\n",
        "\n",
        "# Feature preview\n",
        "df[[\"title\", \"title_log_perplexity\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "y5JkLpNmu-H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 21\n",
        "print(\"cell started\")\n",
        "# Title semantic embeddings → 32 PCs (streamed: memmap + IncrementalPCA)\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    !pip install -q sentence-transformers\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "\n",
        "try:\n",
        "    from sklearn.decomposition import IncrementalPCA\n",
        "except Exception:\n",
        "    !pip install -q scikit-learn\n",
        "    from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "import numpy as np, pandas as pd, torch, os\n",
        "from pathlib import Path\n",
        "\n",
        "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"  # 768-D\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "sbert = SentenceTransformer(MODEL_NAME, device=device)\n",
        "\n",
        "titles = df[\"title\"].astype(str).fillna(\"\").tolist()\n",
        "N = len(titles); D = 768; K = 32\n",
        "EMB_PATH = CACHE / \"title_emb_fp32.dat\"\n",
        "PCS_PATH = CACHE / \"title_pcs_fp32.dat\"\n",
        "\n",
        "# --- Pass 1: write embeddings to a memmap in batches ---\n",
        "if not EMB_PATH.exists():\n",
        "    emb_mm = np.memmap(str(EMB_PATH), dtype=np.float32, mode=\"w+\", shape=(N, D))\n",
        "    bs = 256\n",
        "    w = 0\n",
        "    while w < N:\n",
        "        batch = titles[w:w+bs]\n",
        "        vecs = sbert.encode(\n",
        "            batch,\n",
        "            batch_size=min(64, len(batch)),\n",
        "            show_progress_bar=False,\n",
        "            convert_to_numpy=True,\n",
        "            normalize_embeddings=True,  # already L2-normalized\n",
        "        ).astype(np.float32)\n",
        "        emb_mm[w:w+len(batch)] = vecs\n",
        "        emb_mm.flush()\n",
        "        w += len(batch)\n",
        "    del emb_mm\n",
        "\n",
        "# --- Pass 2: fit IncrementalPCA over the memmap ---\n",
        "emb_mm = np.memmap(str(EMB_PATH), dtype=np.float32, mode=\"r\", shape=(N, D))\n",
        "ipca = IncrementalPCA(n_components=K, batch_size=512)\n",
        "for s in range(0, N, 2048):\n",
        "    ipca.partial_fit(emb_mm[s:s+2048])\n",
        "\n",
        "# --- Pass 3: transform to PCs in streamed chunks, write to memmap ---\n",
        "pcs_mm = np.memmap(str(PCS_PATH), dtype=np.float32, mode=\"w+\", shape=(N, K))\n",
        "for s in range(0, N, 2048):\n",
        "    pcs_mm[s:s+2048] = ipca.transform(emb_mm[s:s+2048]).astype(np.float32)\n",
        "    pcs_mm.flush()\n",
        "\n",
        "# Attach to df\n",
        "pc_cols = [f\"title_emb_pca_{i:02d}\" for i in range(1, K+1)]\n",
        "for c in pc_cols:\n",
        "    if c in df.columns:\n",
        "        df.drop(columns=[c], inplace=True)\n",
        "\n",
        "df[pc_cols] = pd.DataFrame(np.asarray(pcs_mm), index=df.index)\n",
        "\n",
        "explained = float(np.sum(ipca.explained_variance_ratio_) * 100.0)\n",
        "print(f\"IncrementalPCA: kept {K} components explaining {explained:.1f}% of variance.\")\n",
        "print(\"Top 5 component variances:\", np.round(ipca.explained_variance_ratio_[:5], 4))\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "BbHbwO8rvOq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 22\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# title_thumbnail_semantic_alignment (streamed, disk-backed; resume-safe)\n",
        "\n",
        "# Install/open open_clip\n",
        "try:\n",
        "    import open_clip\n",
        "except Exception:\n",
        "    !pip install -q open_clip_torch\n",
        "    import open_clip\n",
        "\n",
        "import torch, numpy as np, os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- Resolve per-row thumbnail paths once ---\n",
        "def _resolve_thumb_path_row(row):\n",
        "    # Prefer explicit thumbnail_path\n",
        "    p = None\n",
        "    tp = row.get(\"thumbnail_path\", None) if isinstance(row, dict) else None\n",
        "    if isinstance(tp, str) and os.path.isfile(tp):\n",
        "        p = tp\n",
        "    elif \"id\" in row and isinstance(row[\"id\"], str):\n",
        "        pp = os.path.join(\"thumbnails\", f\"{row['id']}.jpg\")\n",
        "        if os.path.isfile(pp):\n",
        "            p = pp\n",
        "    return p\n",
        "\n",
        "# .to_dict(\"records\") keeps this fast without copying big objects\n",
        "paths = [ _resolve_thumb_path_row(r) for r in df.to_dict(\"records\") ]\n",
        "titles = df[\"title\"].astype(str).fillna(\"\").tolist()\n",
        "N = len(df)\n",
        "\n",
        "# --- Output memmap on disk (float16 to save space) ---\n",
        "CACHE.mkdir(exist_ok=True)\n",
        "SIM_PATH = CACHE / \"clip_align_fp16.dat\"\n",
        "if not SIM_PATH.exists():\n",
        "    sim_mm = np.memmap(str(SIM_PATH), dtype=np.float16, mode=\"w+\", shape=(N,))\n",
        "    sim_mm[:] = np.nan\n",
        "    sim_mm.flush()\n",
        "else:\n",
        "    sim_mm = np.memmap(str(SIM_PATH), dtype=np.float16, mode=\"r+\", shape=(N,))\n",
        "\n",
        "# --- Load model & tokenizer once ---\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\", device=device)\n",
        "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
        "model.eval()\n",
        "\n",
        "BATCH = 128  # lower if you see VRAM pressure (e.g., 64 or 32)\n",
        "\n",
        "for s, e in chunker(N, BATCH):\n",
        "    # Skip batch if already filled (resume support)\n",
        "    if np.isfinite(np.asarray(sim_mm[s:e])).all():\n",
        "        continue\n",
        "\n",
        "    batch_titles = titles[s:e]\n",
        "    batch_paths  = paths[s:e]\n",
        "\n",
        "    # Prepare text tokens for this batch only\n",
        "    with torch.no_grad():\n",
        "        text_tokens = tokenizer(batch_titles)\n",
        "        text_tokens = torch.tensor(text_tokens, device=device)\n",
        "\n",
        "    # Prepare images for this batch only (keep index mapping for Nones)\n",
        "    imgs, keep_local_idx = [], []\n",
        "    for i, p in enumerate(batch_paths):\n",
        "        if isinstance(p, str):\n",
        "            try:\n",
        "                imgs.append(preprocess(Image.open(p).convert(\"RGB\")))\n",
        "                keep_local_idx.append(i)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # Encode\n",
        "    with torch.inference_mode():\n",
        "        # Text emb (normalized)\n",
        "        t = model.encode_text(text_tokens)\n",
        "        t = t / t.norm(dim=1, keepdim=True).clamp_min(1e-12)\n",
        "\n",
        "        # Image emb (normalized) only for rows that had valid images\n",
        "        if keep_local_idx:\n",
        "            imgs = torch.stack(imgs).to(device)\n",
        "            with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
        "                im = model.encode_image(imgs)\n",
        "            im = im / im.norm(dim=1, keepdim=True).clamp_min(1e-12)\n",
        "        else:\n",
        "            im = None\n",
        "\n",
        "    # Default: NaNs\n",
        "    sims = np.full((e - s,), np.nan, dtype=np.float32)\n",
        "\n",
        "    if keep_local_idx:\n",
        "        # Cosine (dot) since normalized; map [-1,1] -> [0,1]\n",
        "        # Align text rows (keep_local_idx) with image batch rows (0..len-1)\n",
        "        with torch.no_grad():\n",
        "            cos = (t[keep_local_idx] * im).sum(dim=1)\n",
        "            sim01 = ((cos + 1.0) / 2.0).detach().cpu().numpy().astype(np.float32)\n",
        "        for j_local, val in zip(keep_local_idx, sim01):\n",
        "            sims[j_local] = val\n",
        "\n",
        "    # Flush this slice to disk (float16)\n",
        "    sim_mm[s:e] = sims.astype(np.float16)\n",
        "    sim_mm.flush()\n",
        "\n",
        "    # Clean up the batch to keep RAM low\n",
        "    del text_tokens, t, imgs, im, sims\n",
        "    torch_cleanup()\n",
        "\n",
        "# Load into DataFrame (promote back to float32)\n",
        "df[\"title_thumbnail_semantic_alignment\"] = np.asarray(sim_mm, dtype=np.float16).astype(np.float32)\n",
        "print(\"Computed alignment for\", int(np.isfinite(df[\"title_thumbnail_semantic_alignment\"]).sum()), \"rows\")\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "DuylqO_gvskb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 23\n",
        "print(\"cell started\")\n",
        "##### Sanity check + feature peek for next section\n",
        "df.columns\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "Djl75TKXxwCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Tag-level Features"
      ],
      "metadata": {
        "id": "9RA9isoBwn5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 24\n",
        "print(\"cell started\")\n",
        "##### Sanity check + feature peek\n",
        "df.columns\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "-HeT_2Jn8qt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 25\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# tag_semantic_entropy\n",
        "\n",
        "# Deps\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    !pip install -q sentence-transformers\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "\n",
        "try:\n",
        "    from sklearn.cluster import MiniBatchKMeans\n",
        "except Exception:\n",
        "    !pip install -q scikit-learn\n",
        "    from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast, re, torch\n",
        "\n",
        "# Parse and normalize tags per row\n",
        "def parse_tags_cell(x):\n",
        "    # Input can be a list or a string (JSON-ish list, or comma/pipe/semicolon separated)\n",
        "    if isinstance(x, list):\n",
        "        tags = x\n",
        "    elif isinstance(x, str):\n",
        "        s = x.strip()\n",
        "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
        "            try:\n",
        "                parsed = ast.literal_eval(s)\n",
        "                tags = parsed if isinstance(parsed, list) else [s]\n",
        "            except Exception:\n",
        "                tags = re.split(r\"[,\\|;]\", s)\n",
        "        else:\n",
        "            tags = re.split(r\"[,\\|;]\", s)\n",
        "    else:\n",
        "        tags = []\n",
        "\n",
        "    # normalize: lowercase, strip whitespace/hashtags, drop empties; de-dup preserving order\n",
        "    cleaned, seen = [], set()\n",
        "    for t in tags:\n",
        "        if not isinstance(t, str):\n",
        "            continue\n",
        "        tt = t.strip().lower().strip(\"#\")\n",
        "        if tt and tt not in seen:\n",
        "            cleaned.append(tt); seen.add(tt)\n",
        "    return cleaned\n",
        "\n",
        "tag_lists = df[\"tags\"].apply(parse_tags_cell)\n",
        "\n",
        "# Build the unique tag vocabulary\n",
        "all_tags = sorted({t for tags in tag_lists for t in tags})\n",
        "print(f\"Unique tags discovered: {len(all_tags)}\")\n",
        "\n",
        "# Early exit if no tags at all\n",
        "if len(all_tags) == 0:\n",
        "    df[\"tag_semantic_entropy\"] = np.nan\n",
        "else:\n",
        "    # Embed tags with a lightweight SBERT (384-D)\n",
        "    EMB_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # good for short tags\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    encoder = SentenceTransformer(EMB_MODEL, device=device)\n",
        "\n",
        "    def encode_texts(texts, batch_size=512):\n",
        "        out = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i+batch_size]\n",
        "            vecs = encoder.encode(\n",
        "                batch,\n",
        "                batch_size=min(batch_size, len(batch)),\n",
        "                show_progress_bar=False,\n",
        "                convert_to_numpy=True,\n",
        "                normalize_embeddings=True,  # L2 norm for cosine-friendly space\n",
        "            )\n",
        "            out.append(vecs)\n",
        "        return np.vstack(out)\n",
        "\n",
        "    tag_emb = encode_texts(all_tags)  # shape [V, D]\n",
        "\n",
        "    # Global clustering over tag embeddings\n",
        "    K = 32  # tune 16–64; 32 is a good default\n",
        "    kmeans = MiniBatchKMeans(\n",
        "        n_clusters=K,\n",
        "        batch_size=2048,\n",
        "        random_state=42,\n",
        "        n_init=\"auto\",\n",
        "        verbose=0,\n",
        "    ).fit(tag_emb)\n",
        "\n",
        "    # Map each tag to a cluster id (0..K-1)\n",
        "    tag_to_cluster = {t: int(c) for t, c in zip(all_tags, kmeans.predict(tag_emb))}\n",
        "\n",
        "    # Per-row normalized entropy in [0,1]\n",
        "    def row_entropy(tags):\n",
        "        if not tags:\n",
        "            return np.nan\n",
        "        clusters = [tag_to_cluster[t] for t in tags if t in tag_to_cluster]\n",
        "        if not clusters:\n",
        "            return np.nan\n",
        "        counts = np.bincount(clusters, minlength=K).astype(np.float32)\n",
        "        active = counts[counts > 0]\n",
        "        if active.size <= 1:\n",
        "            return 0.0  # all tags in one cluster → zero diversity\n",
        "        p = active / active.sum()\n",
        "        H = -np.sum(p * np.log2(p))   # bits\n",
        "        H_max = np.log2(active.size)  # max bits for this many active clusters\n",
        "        return float(H / H_max) if H_max > 0 else 0.0\n",
        "\n",
        "    df[\"tag_semantic_entropy\"] = tag_lists.apply(row_entropy)\n",
        "\n",
        "# Feature preview\n",
        "df[[\"tags\", \"tag_semantic_entropy\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "aboWAxr4DQgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 26\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# tag_title_coherence\n",
        "\n",
        "# Deps\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    !pip install -q sentence-transformers\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast, re, torch\n",
        "\n",
        "# Parse & normalize tags per row\n",
        "def parse_tags_cell(x):\n",
        "    if isinstance(x, list):\n",
        "        tags = x\n",
        "    elif isinstance(x, str):\n",
        "        s = x.strip()\n",
        "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
        "            try:\n",
        "                parsed = ast.literal_eval(s)\n",
        "                tags = parsed if isinstance(parsed, list) else [s]\n",
        "            except Exception:\n",
        "                tags = re.split(r\"[,\\|;]\", s)\n",
        "        else:\n",
        "            tags = re.split(r\"[,\\|;]\", s)\n",
        "    else:\n",
        "        tags = []\n",
        "\n",
        "    cleaned, seen = [], set()\n",
        "    for t in tags:\n",
        "        if not isinstance(t, str):\n",
        "            continue\n",
        "        tt = t.strip().lower().strip(\"#\")\n",
        "        if tt and tt not in seen:\n",
        "            cleaned.append(tt); seen.add(tt)\n",
        "    return cleaned\n",
        "\n",
        "tag_lists = df[\"tags\"].apply(parse_tags_cell)\n",
        "\n",
        "# Collect unique tags; early-exit if none\n",
        "unique_tags = sorted({t for tags in tag_lists for t in tags})\n",
        "if len(unique_tags) == 0:\n",
        "    df[\"tag_title_coherence\"] = np.nan\n",
        "else:\n",
        "    # Load encoder (same model for titles & tags)\n",
        "    MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # fast, good for short text\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    encoder = SentenceTransformer(MODEL, device=device)\n",
        "\n",
        "    def encode_texts(texts, batch_size=512):\n",
        "        if len(texts) == 0:\n",
        "            return np.empty((0, 384), dtype=np.float32)\n",
        "        return encoder.encode(\n",
        "            texts,\n",
        "            batch_size=min(batch_size, len(texts)),\n",
        "            show_progress_bar=False,\n",
        "            convert_to_numpy=True,\n",
        "            normalize_embeddings=True,  # L2-normalized outputs\n",
        "        )\n",
        "\n",
        "    # Embed unique tags once, build tag->vector map\n",
        "    tag_vecs = encode_texts(unique_tags, batch_size=1024)\n",
        "    tag2vec = {t: v for t, v in zip(unique_tags, tag_vecs)}\n",
        "\n",
        "    # Embed all titles\n",
        "    titles = df[\"title\"].astype(str).fillna(\"\").tolist()\n",
        "    title_blank = [not t.strip() for t in titles]\n",
        "    title_vecs = encode_texts(titles, batch_size=256)  # already normalized\n",
        "\n",
        "    # Per-row mean tag embedding (normalized), then cosine with title\n",
        "    def mean_tag_vec(tags_for_row):\n",
        "        vecs = [tag2vec[t] for t in tags_for_row if t in tag2vec]\n",
        "        if not vecs:\n",
        "            return None\n",
        "        m = np.mean(vecs, axis=0)\n",
        "        n = np.linalg.norm(m)\n",
        "        if n == 0:\n",
        "            return None\n",
        "        return m / n\n",
        "\n",
        "    tag_means = [mean_tag_vec(tags) for tags in tag_lists]\n",
        "\n",
        "    sims_01 = []\n",
        "    for i, (tvec, mvec) in enumerate(zip(title_vecs, tag_means)):\n",
        "        if mvec is None or title_blank[i]:\n",
        "            sims_01.append(np.nan)\n",
        "        else:\n",
        "            cos = float(np.clip(np.dot(tvec, mvec), -1.0, 1.0))  # cosine since both L2-normalized\n",
        "            sims_01.append((cos + 1.0) / 2.0)  # map [-1,1] -> [0,1]\n",
        "\n",
        "    df[\"tag_title_coherence\"] = sims_01\n",
        "\n",
        "# Feature preview\n",
        "df[[\"tags\", \"title\", \"tag_title_coherence\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "YDGwNajfEVTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 27\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# tag_num_unique\n",
        "# tag_multiword_ratio\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast, re\n",
        "\n",
        "def parse_tags_cell(x):\n",
        "    \"\"\"\n",
        "    Normalize a tags cell into a de-duplicated list of lowercase strings.\n",
        "    Accepts lists or strings (JSON-ish list or comma/pipe/semicolon separated).\n",
        "    Strips leading '#' and whitespace.\n",
        "    \"\"\"\n",
        "    if isinstance(x, list):\n",
        "        tags = x\n",
        "    elif isinstance(x, str):\n",
        "        s = x.strip()\n",
        "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
        "            try:\n",
        "                parsed = ast.literal_eval(s)\n",
        "                tags = parsed if isinstance(parsed, list) else [s]\n",
        "            except Exception:\n",
        "                tags = re.split(r\"[,\\|;]\", s)\n",
        "        else:\n",
        "            tags = re.split(r\"[,\\|;]\", s)\n",
        "    else:\n",
        "        tags = []\n",
        "\n",
        "    cleaned, seen = [], set()\n",
        "    for t in tags:\n",
        "        if not isinstance(t, str):\n",
        "            continue\n",
        "        tt = t.strip().lower().strip(\"#\")\n",
        "        if tt and tt not in seen:\n",
        "            cleaned.append(tt); seen.add(tt)\n",
        "    return cleaned\n",
        "\n",
        "def is_multiword(tag: str) -> bool:\n",
        "    \"\"\"\n",
        "    Heuristic: a tag has >1 'word' if splitting on whitespace/underscores/hyphens yields >1 tokens.\n",
        "    \"\"\"\n",
        "    tokens = [tok for tok in re.split(r\"[\\s\\-_]+\", tag) if tok]\n",
        "    return len(tokens) > 1\n",
        "\n",
        "# Parse tags once\n",
        "_tag_lists = df[\"tags\"].apply(parse_tags_cell)\n",
        "\n",
        "# Feature: number of unique tags\n",
        "df[\"tag_num_unique\"] = _tag_lists.apply(len).astype(\"Int64\")\n",
        "\n",
        "# Feature: ratio of multiword tags among unique tags\n",
        "def multiword_ratio(tags):\n",
        "    if not tags:\n",
        "        return np.nan\n",
        "    mw = sum(1 for t in tags if is_multiword(t))\n",
        "    return mw / len(tags)\n",
        "\n",
        "df[\"tag_multiword_ratio\"] = _tag_lists.apply(multiword_ratio).astype(float)\n",
        "\n",
        "# Feature preview\n",
        "df[[\"tags\", \"tag_num_unique\", \"tag_multiword_ratio\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "8RkVbmOgFWgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 28\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# tag_title_overlap\n",
        "\n",
        "# Deps: nltk only for PorterStemmer (no corpora), sklearn for stopwords\n",
        "try:\n",
        "    from nltk.stem import PorterStemmer\n",
        "except Exception:\n",
        "    !pip install -q nltk\n",
        "    from nltk.stem import PorterStemmer\n",
        "\n",
        "try:\n",
        "    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "except Exception:\n",
        "    !pip install -q scikit-learn\n",
        "    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "import re, ast, numpy as np, pandas as pd\n",
        "\n",
        "# Stopword set (sklearn) + a few domain extras\n",
        "STOP = set(ENGLISH_STOP_WORDS)\n",
        "STOP |= {\"video\",\"videos\",\"official\",\"channel\",\"new\",\"best\",\"top\",\"full\",\"live\",\"shorts\"}\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "_WORD_RE = re.compile(r\"[a-z0-9]+\")\n",
        "\n",
        "def parse_tags_cell(x):\n",
        "    if isinstance(x, list):\n",
        "        tags = x\n",
        "    elif isinstance(x, str):\n",
        "        s = x.strip()\n",
        "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
        "            try:\n",
        "                parsed = ast.literal_eval(s)\n",
        "                tags = parsed if isinstance(parsed, list) else [s]\n",
        "            except Exception:\n",
        "                tags = re.split(r\"[,\\|;]\", s)\n",
        "        else:\n",
        "            tags = re.split(r\"[,\\|;]\", s)\n",
        "    else:\n",
        "        tags = []\n",
        "    cleaned, seen = [], set()\n",
        "    for t in tags:\n",
        "        if not isinstance(t, str):\n",
        "            continue\n",
        "        tt = t.strip().lower().strip(\"#\")\n",
        "        if tt and tt not in seen:\n",
        "            cleaned.append(tt); seen.add(tt)\n",
        "    return cleaned\n",
        "\n",
        "def tokenize_normalize(text: str):\n",
        "    if not isinstance(text, str):\n",
        "        return set()\n",
        "    toks = _WORD_RE.findall(text.lower())\n",
        "    toks = [t for t in toks if t and t not in STOP]\n",
        "    return {stemmer.stem(t) for t in toks}\n",
        "\n",
        "def tags_to_token_set(tags_list):\n",
        "    tokens = []\n",
        "    for tag in tags_list:\n",
        "        for part in re.split(r\"[\\s\\-_]+\", tag):\n",
        "            if part:\n",
        "                tokens.extend(_WORD_RE.findall(part.lower()))\n",
        "    tokens = [t for t in tokens if t and t not in STOP]\n",
        "    return {stemmer.stem(t) for t in tokens}\n",
        "\n",
        "def jaccard(a: set, b: set):\n",
        "    if not a or not b:\n",
        "        return np.nan  # use 0.0 instead if you prefer\n",
        "    inter = len(a & b)\n",
        "    union = len(a | b)\n",
        "    return inter / union if union else np.nan\n",
        "\n",
        "# Compute feature\n",
        "_tag_lists = df[\"tags\"].apply(parse_tags_cell)\n",
        "_title_sets = df[\"title\"].astype(str).apply(tokenize_normalize)\n",
        "_tag_sets = _tag_lists.apply(tags_to_token_set)\n",
        "\n",
        "df[\"tag_title_overlap\"] = [jaccard(a, b) for a, b in zip(_title_sets, _tag_sets)]\n",
        "\n",
        "# Feature preview\n",
        "df[[\"title\", \"tags\", \"tag_title_overlap\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "1Rpz0S3nG2Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Additional Cross-level Features"
      ],
      "metadata": {
        "id": "ypwBULDCHuz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 29\n",
        "print(\"cell started\")\n",
        "##### Sanity check + feature peek\n",
        "df.columns\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "kqTsPpgNyhCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 30\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# Cross-level feature between title and face sentiment\n",
        "\n",
        "# title_face_sentiment_gap               = title_sentiment - thumbnail_face_emotion           [-2,2]\n",
        "# title_face_sentiment_mismatch          = abs(gap)                                           [0,2]\n",
        "# title_face_sentiment_mismatch_weighted = mismatch * thumbnail_face_area_ratio               [0,2] scaled by face area\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure required columns exist and are numeric\n",
        "for col in [\"title_sentiment\", \"thumbnail_face_emotion\", \"thumbnail_face_area_ratio\"]:\n",
        "    if col not in df.columns:\n",
        "        raise KeyError(f\"Missing required column: {col}\")\n",
        "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "# Signed gap (direction matters)\n",
        "df[\"title_face_sentiment_gap\"] = df[\"title_sentiment\"] - df[\"thumbnail_face_emotion\"]\n",
        "\n",
        "# Magnitude-only mismatch\n",
        "df[\"title_face_sentiment_mismatch\"] = df[\"title_face_sentiment_gap\"].abs()\n",
        "\n",
        "# Weighted by face presence/size (zeros out when no face; stays NaN if inputs are NaN)\n",
        "df[\"title_face_sentiment_mismatch_weighted\"] = (\n",
        "    df[\"title_face_sentiment_mismatch\"] * df[\"thumbnail_face_area_ratio\"].fillna(0.0)\n",
        ")\n",
        "\n",
        "# Feature preview\n",
        "df[[\n",
        "    \"title\", \"thumbnail_face_emotion\", \"thumbnail_face_area_ratio\",\n",
        "    \"title_sentiment\", \"title_face_sentiment_gap\",\n",
        "    \"title_face_sentiment_mismatch\", \"title_face_sentiment_mismatch_weighted\"\n",
        "]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "XwanramTnUbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 31\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# Coherence Triangle: Title–Tags–Image\n",
        "\n",
        "# tri_coherence = tag_title_coherence × title_thumbnail_semantic_alignment    [0,1]\n",
        "# tri_coherence_lex = tri_coherence × tag_title_overlap                       [0,1]\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Verify required columns\n",
        "need = [\"tag_title_coherence\", \"title_thumbnail_semantic_alignment\"]\n",
        "missing = [c for c in need if c not in df.columns]\n",
        "if missing:\n",
        "    raise KeyError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "# Coerce to numeric\n",
        "df[\"tag_title_coherence\"] = pd.to_numeric(df[\"tag_title_coherence\"], errors=\"coerce\")\n",
        "df[\"title_thumbnail_semantic_alignment\"] = pd.to_numeric(df[\"title_thumbnail_semantic_alignment\"], errors=\"coerce\")\n",
        "\n",
        "# Main triangle coherence (Title–Tags × Title–Image)\n",
        "df[\"tri_coherence\"] = df[\"tag_title_coherence\"] * df[\"title_thumbnail_semantic_alignment\"]\n",
        "\n",
        "# Optional lexical-enhanced version if overlap column exists\n",
        "overlap_col = None\n",
        "if \"tag_title_overlap_jaccard\" in df.columns:\n",
        "    overlap_col = \"tag_title_overlap_jaccard\"\n",
        "elif \"tag_title_overlap\" in df.columns:\n",
        "    overlap_col = \"tag_title_overlap\"\n",
        "\n",
        "if overlap_col:\n",
        "    df[overlap_col] = pd.to_numeric(df[overlap_col], errors=\"coerce\")\n",
        "    df[\"tri_coherence_lex\"] = df[\"tri_coherence\"] * df[overlap_col]\n",
        "    print(f\"Created tri_coherence_lex using overlap column: {overlap_col}\")\n",
        "\n",
        "# Quick peek\n",
        "cols = [\"title\", \"tag_title_coherence\", \"title_thumbnail_semantic_alignment\", \"tri_coherence\"]\n",
        "if \"tri_coherence_lex\" in df.columns:\n",
        "    cols.append(\"tri_coherence_lex\")\n",
        "df[cols].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "ddV4E2Blxz-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 32\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# Text-heavy x Readability cross-level feature interaction\n",
        "\n",
        "# text_x_hard       -> thumbnail_ocr_text_coverage * (100 - title_readability)         [0,100]\n",
        "# text_x_hard_norm  -> normalized version                                              [0,1]\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure required columns exist and are numeric\n",
        "need = [\"thumbnail_ocr_text_coverage\", \"title_readability\"]\n",
        "missing = [c for c in need if c not in df.columns]\n",
        "if missing:\n",
        "    raise KeyError(f\"Missing required columns: {missing}\")\n",
        "df[need] = df[need].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "# Clip to expected ranges\n",
        "cov = df[\"thumbnail_ocr_text_coverage\"].clip(0.0, 1.0)\n",
        "fre = df[\"title_readability\"].clip(0.0, 100.0)  # Flesch Reading Ease (0..100, higher = easier)\n",
        "\n",
        "# Interaction: more text + harder-to-read title → higher risk\n",
        "df[\"text_x_hard\"] = cov * (100.0 - fre)          # 0..100\n",
        "df[\"text_x_hard_norm\"] = df[\"text_x_hard\"] / 100.0  # 0..1\n",
        "\n",
        "# Feature preview\n",
        "df[[\"title\", \"thumbnail_ocr_text_coverage\", \"title_readability\",\n",
        "    \"text_x_hard\", \"text_x_hard_norm\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "HAD-W6OkBFZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 33\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# Complexity × Text interactions\n",
        "\n",
        "# complexity_x_text_edge     -> edge density × OCR text coverage          [0,1]\n",
        "# complexity_x_text_entropy  -> normalized texture entropy × text cover   [0,1]\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Ensure required columns\n",
        "need = [\"thumbnail_edge_density\", \"thumbnail_texture_entropy\", \"thumbnail_ocr_text_coverage\"]\n",
        "missing = [c for c in need if c not in df.columns]\n",
        "if missing:\n",
        "    raise KeyError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "# Coerce & clip to expected ranges\n",
        "edge = pd.to_numeric(df[\"thumbnail_edge_density\"], errors=\"coerce\").clip(0.0, 1.0)\n",
        "ent  = pd.to_numeric(df[\"thumbnail_texture_entropy\"], errors=\"coerce\").clip(lower=0.0)   # bits, usually 0..8\n",
        "cov  = pd.to_numeric(df[\"thumbnail_ocr_text_coverage\"], errors=\"coerce\").clip(0.0, 1.0)\n",
        "\n",
        "# Normalize entropy to [0,1] assuming 8-bit grayscale (max entropy ≈ 8 bits)\n",
        "MAX_BITS = 8.0\n",
        "ent_norm = (ent / MAX_BITS).clip(0.0, 1.0)\n",
        "\n",
        "# Interactions\n",
        "df[\"complexity_x_text_edge\"]    = edge * cov                 # busy background + text → legibility risk\n",
        "df[\"complexity_x_text_entropy\"] = ent_norm * cov             # texture complexity + text → legibility risk\n",
        "\n",
        "# Feature preview\n",
        "df[[\n",
        "    \"title\", \"thumbnail_edge_density\", \"thumbnail_texture_entropy\", \"thumbnail_ocr_text_coverage\",\n",
        "    \"complexity_x_text_edge\", \"complexity_x_text_entropy\"\n",
        "]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "1W6ULunwBeaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 34\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# Composition × Face interaction\n",
        "\n",
        "#   face_comp     -> thumbnail_face_area_ratio * thumbnail_saliency_thirds_proximity  (0..1)\n",
        "#   face_present  -> 1 if any face area > 0 else 0 (helper flag)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure required columns\n",
        "need = [\"thumbnail_face_area_ratio\", \"thumbnail_saliency_thirds_proximity\"]\n",
        "missing = [c for c in need if c not in df.columns]\n",
        "if missing:\n",
        "    raise KeyError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "# Coerce & clip\n",
        "face_area = pd.to_numeric(df[\"thumbnail_face_area_ratio\"], errors=\"coerce\").clip(0.0, 1.0).fillna(0.0)\n",
        "thirds    = pd.to_numeric(df[\"thumbnail_saliency_thirds_proximity\"], errors=\"coerce\").clip(0.0, 1.0)\n",
        "\n",
        "# Interaction: bigger faces placed closer to rule-of-thirds hotspots → higher score\n",
        "df[\"face_comp\"] = face_area * thirds  # stays in [0,1] if inputs are\n",
        "\n",
        "# Optional helper: face presence indicator\n",
        "df[\"face_present\"] = (face_area > 0).astype(int)\n",
        "\n",
        "# Feature preview\n",
        "df[[\"title\", \"thumbnail_face_area_ratio\", \"thumbnail_saliency_thirds_proximity\", \"face_comp\", \"face_present\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "0Twv9XdnCXdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 35\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# Warmth/Coolness × Sentiment cross-level feature interactions\n",
        "\n",
        "# warmth_score        -> closeness to warm hues, scaled by saturation                [0,1]\n",
        "# cool_score          -> closeness to cool hues (~blue/cyan), scaled by saturation   [0,1]\n",
        "# warm_x_positive     -> warmth × (joy + positive VADER)                             [0,1] normalized\n",
        "# cool_x_negative     -> coolness × (sadness + fear + neg VADER)                     [0,1] normalized\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure required columns\n",
        "need = [\"thumbnail_hue\", \"thumbnail_saturation\",\n",
        "        \"title_emotion_joy\", \"title_emotion_sadness\", \"title_emotion_fear\",\n",
        "        \"title_sentiment\"]\n",
        "missing = [c for c in need if c not in df.columns]\n",
        "if missing:\n",
        "    raise KeyError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "df[need] = df[need].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "# Normalize hue to degrees and saturation to [0,1] (robust to different scales)\n",
        "def norm_hue_to_degrees(series: pd.Series) -> pd.Series:\n",
        "    m = series.max(skipna=True)\n",
        "    if pd.isna(m):\n",
        "        return series\n",
        "    if m <= 1.5:        # 0..1 -> degrees\n",
        "        out = (series * 360.0) % 360.0\n",
        "    elif m <= 190:      # OpenCV 0..179 -> degrees\n",
        "        out = (series * 2.0) % 360.0\n",
        "    else:               # already degrees\n",
        "        out = series % 360.0\n",
        "    return out\n",
        "\n",
        "def norm_sat_to_unit(series: pd.Series) -> pd.Series:\n",
        "    m = series.max(skipna=True)\n",
        "    if pd.isna(m):\n",
        "        return series\n",
        "    if m <= 1.5:             # 0..1\n",
        "        out = series.clip(0.0, 1.0)\n",
        "    elif m <= 255 + 1e-6:    # 0..255\n",
        "        out = (series / 255.0).clip(0.0, 1.0)\n",
        "    elif m <= 100 + 1e-6:    # 0..100\n",
        "        out = (series / 100.0).clip(0.0, 1.0)\n",
        "    else:                    # fallback min-max\n",
        "        mn = series.min(skipna=True)\n",
        "        rng = max(float(m - mn), 1e-6)\n",
        "        out = ((series - mn) / rng).clip(0.0, 1.0)\n",
        "    return out\n",
        "\n",
        "h_deg = norm_hue_to_degrees(df[\"thumbnail_hue\"].astype(float))\n",
        "s01  = norm_sat_to_unit(df[\"thumbnail_saturation\"].astype(float)).fillna(0.0)\n",
        "\n",
        "# Warmth & Coolness scores (0..1), modulated by saturation\n",
        "# Warmth: closeness to warm band around 0° (reds/oranges/yellows), zero by ±60°\n",
        "d_warm = np.minimum(h_deg, 360.0 - h_deg)  # angular distance to 0° with wrap\n",
        "warm_closeness = np.maximum(0.0, 1.0 - (d_warm / 60.0))  # 1 at 0°, 0 by 60°\n",
        "warmth = (warm_closeness * s01).clip(0.0, 1.0)\n",
        "\n",
        "# Coolness: closeness to 180° (cyan/blue), zero by ±60°\n",
        "delta = np.abs(h_deg - 180.0)\n",
        "d_cool = np.minimum(delta, 360.0 - delta)  # angular distance to 180° with wrap\n",
        "cool_closeness = np.maximum(0.0, 1.0 - (d_cool / 60.0))  # 1 at 180°, 0 by 60°\n",
        "coolness = (cool_closeness * s01).clip(0.0, 1.0)\n",
        "\n",
        "df[\"warmth_score\"] = warmth\n",
        "df[\"cool_score\"]   = coolness\n",
        "\n",
        "# Sentiment parts (use probabilities + VADER parts)\n",
        "joy = df[\"title_emotion_joy\"].fillna(0.0).clip(0.0, 1.0)\n",
        "sad = df[\"title_emotion_sadness\"].fillna(0.0).clip(0.0, 1.0)\n",
        "fear = df[\"title_emotion_fear\"].fillna(0.0).clip(0.0, 1.0)\n",
        "sent = df[\"title_sentiment\"].fillna(0.0)\n",
        "\n",
        "pos_part = sent.clip(lower=0.0)          # positive VADER\n",
        "neg_part = (-sent).clip(lower=0.0)       # negative VADER\n",
        "\n",
        "# Raw sums could exceed 1; we provide normalized [0,1] features\n",
        "# Warm × positive: warmth * (joy + pos_sent) / 2\n",
        "df[\"warm_x_positive\"] = (warmth * (joy + pos_part) / 2.0).clip(0.0, 1.0)\n",
        "\n",
        "# Cool × negative: cool * (sadness + fear + neg_sent) / 3\n",
        "df[\"cool_x_negative\"] = (coolness * (sad + fear + neg_part) / 3.0).clip(0.0, 1.0)\n",
        "\n",
        "# Quick peek\n",
        "df[[\n",
        "    \"title\", \"thumbnail_hue\", \"thumbnail_saturation\",\n",
        "    \"warmth_score\", \"cool_score\",\n",
        "    \"warm_x_positive\", \"cool_x_negative\"\n",
        "]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "WpfJEq2IDDxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 36\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# Novel phrasing × Alignment cross-level feature interactions\n",
        "\n",
        "#   novelty_score       -> normalized title novelty (from title_log_perplexity)     [0,1]\n",
        "#   novel_x_align       -> novelty × alignment (beneficial grounded novelty)\n",
        "#   novel_x_misaligned  -> novelty × (1 - alignment) (risky novelty)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure required columns\n",
        "need = [\"title_log_perplexity\", \"title_thumbnail_semantic_alignment\"]\n",
        "missing = [c for c in need if c not in df.columns]\n",
        "if missing:\n",
        "    raise KeyError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "# Coerce to numeric\n",
        "lp  = pd.to_numeric(df[\"title_log_perplexity\"], errors=\"coerce\")                 # nats/token\n",
        "ali = pd.to_numeric(df[\"title_thumbnail_semantic_alignment\"], errors=\"coerce\")   # expected 0..1\n",
        "\n",
        "# Normalize novelty to [0,1] robustly using 5th–95th percentiles (clip outliers)\n",
        "p5, p95 = np.nanpercentile(lp, [5, 95]) if lp.notna().any() else (0.0, 1.0)\n",
        "rng = max(p95 - p5, 1e-6)\n",
        "novelty = ((lp - p5) / rng).clip(0.0, 1.0)\n",
        "\n",
        "# Clip alignment to [0,1]\n",
        "ali = ali.clip(0.0, 1.0)\n",
        "\n",
        "# Interactions\n",
        "df[\"novelty_score\"] = novelty\n",
        "df[\"novel_x_align\"] = novelty * ali                 # novelty that is grounded by a well-aligned image\n",
        "df[\"novel_x_misaligned\"] = novelty * (1.0 - ali)    # novelty that is NOT grounded (riskier)\n",
        "\n",
        "# Feature preview\n",
        "df[[\"title\", \"title_log_perplexity\", \"title_thumbnail_semantic_alignment\",\n",
        "    \"novelty_score\", \"novel_x_align\", \"novel_x_misaligned\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "BjrivVwFEoI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5: Trending Score"
      ],
      "metadata": {
        "id": "Fg6G1_sAG7Kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 37\n",
        "print(\"cell started\")\n",
        "##### Creates:\n",
        "# trending_score ∈ [0,1]\n",
        "\n",
        "# External Trending Score via YouTube Data API (cheap & first-party)\n",
        "# Fetches \"mostPopular\" videos for selected regions\n",
        "# Builds a token→score dict (view-weighted)\n",
        "\n",
        "import os, re, ast, math, requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# CONFIG\n",
        "from google.colab import userdata\n",
        "YOUTUBE_API_KEY = userdata.get('YOUTUBE_API_KEY')\n",
        "\n",
        "REGIONS = [\"US\",\"GB\",\"IN\",\"BR\",\"DE\",\"JP\",\"KR\",\"FR\",\"CA\",\"AU\"]   # tweak as you like\n",
        "PAGES_PER_REGION = 2    # each page returns up to 50 videos → ~100/region\n",
        "TIMEOUT = 20\n",
        "\n",
        "# stopwords (no downloads)\n",
        "try:\n",
        "    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "    STOP = set(ENGLISH_STOP_WORDS)\n",
        "except Exception:\n",
        "    STOP = set()\n",
        "STOP |= {\"video\",\"videos\",\"official\",\"channel\",\"new\",\"best\",\"top\",\"full\",\"live\",\"shorts\"}\n",
        "\n",
        "WORD_RE = re.compile(r\"[a-z0-9]+\")\n",
        "\n",
        "def tokenize_text(s: str):\n",
        "    toks = WORD_RE.findall(str(s).lower())\n",
        "    return [t for t in toks if t and t not in STOP]\n",
        "\n",
        "def parse_tags_cell(x):\n",
        "    if isinstance(x, list):\n",
        "        tags = x\n",
        "    elif isinstance(x, str):\n",
        "        s = x.strip()\n",
        "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
        "            try:\n",
        "                tags = ast.literal_eval(s)\n",
        "                if not isinstance(tags, list): tags = [s]\n",
        "            except Exception:\n",
        "                tags = re.split(r\"[,\\|;]\", s)\n",
        "        else:\n",
        "            tags = re.split(r\"[,\\|;]\", s)\n",
        "    else:\n",
        "        tags = []\n",
        "    # basic normalize\n",
        "    seen, out = set(), []\n",
        "    for t in tags:\n",
        "        if not isinstance(t, str): continue\n",
        "        tt = t.strip().lower().strip(\"#\")\n",
        "        if tt and tt not in seen:\n",
        "            out.append(tt); seen.add(tt)\n",
        "    return out\n",
        "\n",
        "def split_tag_to_tokens(tag: str):\n",
        "    toks = []\n",
        "    for part in re.split(r\"[\\s\\-_]+\", tag):\n",
        "        toks.extend(WORD_RE.findall(part.lower()))\n",
        "    return [t for t in toks if t and t not in STOP]\n",
        "\n",
        "# FETCH TRENDING ITEMS\n",
        "def fetch_trending_items(region: str, pages: int = 2):\n",
        "    url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
        "    items = []\n",
        "    page_token = None\n",
        "    for _ in range(max(1, pages)):\n",
        "        params = {\n",
        "            \"part\": \"snippet,statistics\",\n",
        "            \"chart\": \"mostPopular\",\n",
        "            \"regionCode\": region,\n",
        "            \"maxResults\": 50,\n",
        "            \"pageToken\": page_token or \"\",\n",
        "            \"key\": YOUTUBE_API_KEY\n",
        "        }\n",
        "        r = requests.get(url, params=params, timeout=TIMEOUT)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        items.extend(data.get(\"items\", []))\n",
        "        page_token = data.get(\"nextPageToken\")\n",
        "        if not page_token:\n",
        "            break\n",
        "    return items\n",
        "\n",
        "all_items = []\n",
        "for rc in REGIONS:\n",
        "    try:\n",
        "        all_items.extend(fetch_trending_items(rc, PAGES_PER_REGION))\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] region {rc}: {e}\")\n",
        "\n",
        "print(f\"Fetched {len(all_items)} trending video items across {len(REGIONS)} regions.\")\n",
        "\n",
        "# BUILD TOKEN → SCORE DICT\n",
        "token_scores = Counter()\n",
        "for it in all_items:\n",
        "    snip = it.get(\"snippet\", {})\n",
        "    stats = it.get(\"statistics\", {})\n",
        "    title = snip.get(\"title\", \"\")\n",
        "    tags  = snip.get(\"tags\") or []\n",
        "\n",
        "    # weight by views (log1p to tame extremes)\n",
        "    try:\n",
        "        views = int(stats.get(\"viewCount\", \"0\"))\n",
        "    except Exception:\n",
        "        views = 0\n",
        "    w = math.log1p(max(views, 0))\n",
        "\n",
        "    # tokens from title + tags\n",
        "    toks = set(tokenize_text(title))\n",
        "    for tg in parse_tags_cell(tags):\n",
        "        toks.update(split_tag_to_tokens(tg))\n",
        "\n",
        "    for tok in toks:\n",
        "        token_scores[tok] += w\n",
        "\n",
        "# prune ultra-rare tokens\n",
        "MIN_TOKENS = 3\n",
        "token_scores = Counter({k:v for k,v in token_scores.items() if v > 0 and v >= MIN_TOKENS})\n",
        "\n",
        "print(f\"Trending vocabulary size: {len(token_scores)}\")\n",
        "\n",
        "# Convert to dict of floats\n",
        "tok2score = {k: float(v) for k, v in token_scores.items()}\n",
        "\n",
        "# MAP TO YOUR DF AS trending_score\n",
        "# Prepare per-row token sets from your current df (title + tags)\n",
        "title_tokens = df[\"title\"].astype(str).apply(tokenize_text)\n",
        "if \"tags\" in df.columns:\n",
        "    tag_lists = df[\"tags\"].apply(parse_tags_cell)\n",
        "    tag_tokens = tag_lists.apply(lambda lst: [t for tg in lst for t in split_tag_to_tokens(tg)])\n",
        "else:\n",
        "    tag_tokens = pd.Series([[] for _ in range(len(df))], index=df.index)\n",
        "\n",
        "row_token_sets = [set(a) | set(b) for a,b in zip(title_tokens, tag_tokens)]\n",
        "\n",
        "def topk_mean(scores, k=3):\n",
        "    if not scores:\n",
        "        return np.nan\n",
        "    arr = np.array(scores, dtype=float)\n",
        "    if arr.size >= k:\n",
        "        arr.sort()\n",
        "        return float(arr[-k:].mean())\n",
        "    return float(arr.mean())\n",
        "\n",
        "row_raw = []\n",
        "for toks in row_token_sets:\n",
        "    scores = [tok2score[t] for t in toks if t in tok2score]\n",
        "    row_raw.append(topk_mean(scores, k=3))\n",
        "\n",
        "df[\"trending_score_raw\"] = row_raw\n",
        "\n",
        "# Normalize to [0,1] using robust min/max (5th–95th pct)\n",
        "vals = pd.to_numeric(df[\"trending_score_raw\"], errors=\"coerce\")\n",
        "if vals.notna().any():\n",
        "    p5, p95 = np.nanpercentile(vals, [5, 95])\n",
        "    rng = max(p95 - p5, 1e-6)\n",
        "    df[\"trending_score\"] = ((vals - p5) / rng).clip(0.0, 1.0)\n",
        "else:\n",
        "    df[\"trending_score\"] = np.nan\n",
        "\n",
        "# Optional: drop the raw column after inspection\n",
        "df.drop(columns=[\"trending_score_raw\"], inplace=True)\n",
        "\n",
        "# Feature preview\n",
        "df[[\"title\", \"trending_score\"]].head()\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "nFBcHOCMuzPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 38\n",
        "print(\"cell started\")\n",
        "##### Sanity check + feature peek\n",
        "df.columns\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "UQEUoCg-w-_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 6: Cleanup & Export"
      ],
      "metadata": {
        "id": "FEeZPPvQxfWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 39\n",
        "print(\"cell started\")\n",
        "##### Dropping helper columns\n",
        "\n",
        "# Dropping bad columns/features\n",
        "columns_to_drop = [\n",
        "    'tags',\n",
        "    'title',\n",
        "    'thumbnail_path',\n",
        "    'thumbnail_exists'\n",
        "]\n",
        "\n",
        "# Check which columns exist in the DataFrame before dropping\n",
        "existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
        "\n",
        "df = df.drop(columns=existing_columns_to_drop, axis=1)\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "Zw8ckEoXxiBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 40\n",
        "print(\"cell started\")\n",
        "df.columns\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "zWLxcgWbx9Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 41\n",
        "print(\"cell started\")\n",
        "filename = 'engineered_features_v1.csv'\n",
        "\n",
        "# Save DataFrame to CSV\n",
        "df.to_csv(filename, index=False)\n",
        "\n",
        "# Download it\n",
        "from google.colab import files\n",
        "files.download(filename)\n",
        "print(\"cell complete\")"
      ],
      "metadata": {
        "id": "U-QkaxJsx-cP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}